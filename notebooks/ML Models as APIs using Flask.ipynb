{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning models as APIs\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "\n",
    "\n",
    "#### Environment Setup & Flask Basics\n",
    "\n",
    "- Creating a virtual environment using `Anaconda`. If you need to create your workflows in Python and keep the dependencies separated out or share the environment settings, `Anaconda` distributions are a great option. \n",
    "    * You'll find a miniconda installation for Python [here](https://conda.io/miniconda.html)\n",
    "    * `wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh`\n",
    "    * `bash Miniconda3-latest-Linux-x86_64.sh`\n",
    "    * Follow the sequence of questions.\n",
    "    * `source .bashrc`\n",
    "    * If you run: `conda`, you should be able to get the list of commands & help.\n",
    "    * To create a new environment, run: `conda create --name <environment-name> python=3.6`\n",
    "    * Follow the steps & once done run: `source activate <environment-name>`\n",
    "    * Install the python packages you need, the two important are: `flask` & `gunicorn`.\n",
    "    \n",
    "    \n",
    "- We'll try out a simple `Flask` Hello-World application and serve it using `gunicorn`:\n",
    "\n",
    "```python\n",
    "\n",
    "\"\"\"Filename: hello-world.py\n",
    "\"\"\"\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/users/<string:username>')\n",
    "def hello_world(username=None):\n",
    "    \n",
    "    return(\"Hello {}!\".format(username))\n",
    "\n",
    "```\n",
    "\n",
    "- To serve it, run: `gunicorn --bind 0.0.0.0:8000 hello-world:app` on your terminal.\n",
    "\n",
    "![Hello World](./images/flaskapp1.png)\n",
    "\n",
    "- On you browser, try out: `https://localhost:8000/users/any-name`\n",
    "\n",
    "![Browser](./images/flaskapp2.png)\n",
    "\n",
    "With a few simple steps, we were able to create web-endpoints that can be accessed locally. \n",
    "\n",
    "Using `Flask`, we can wrap our Machine Learning models and serve them as Web APIs easily. Also, if we want to create more complex web applications (that includes JavaScript `*gasps*`) we just need a few modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Machine Learning Model\n",
    "\n",
    "- We'll be taking up the Machine Learning competition: [Loan Prediction Competition](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii). The main objective is to set a pre-processing pipeline and creating ML Models with goal towards making the ML Predictions easy while deployments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the datasets in a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv  training.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/pratos/Side-Project/av_articles/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loan_ID',\n",
       " 'Gender',\n",
       " 'Married',\n",
       " 'Dependents',\n",
       " 'Education',\n",
       " 'Self_Employed',\n",
       " 'ApplicantIncome',\n",
       " 'CoapplicantIncome',\n",
       " 'LoanAmount',\n",
       " 'Loan_Amount_Term',\n",
       " 'Credit_History',\n",
       " 'Property_Area',\n",
       " 'Loan_Status']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finding out the `null/Nan` values in the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in:Loan_ID == 0\n",
      "The number of null values in:Gender == 13\n",
      "The number of null values in:Married == 3\n",
      "The number of null values in:Dependents == 15\n",
      "The number of null values in:Education == 0\n",
      "The number of null values in:Self_Employed == 32\n",
      "The number of null values in:ApplicantIncome == 0\n",
      "The number of null values in:CoapplicantIncome == 0\n",
      "The number of null values in:LoanAmount == 22\n",
      "The number of null values in:Loan_Amount_Term == 14\n",
      "The number of null values in:Credit_History == 50\n",
      "The number of null values in:Property_Area == 0\n",
      "The number of null values in:Loan_Status == 0\n"
     ]
    }
   ],
   "source": [
    "for _ in data.columns:\n",
    "    print(\"The number of null values in:{} == {}\".format(_, data[_].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there are `22 instances` in `LoanAmount` that don't have values. We won't be considering those instances, you could also impute the values and check out if there's any effect on the final model. But for practical scenarios we'll remove those instances.\n",
    "\n",
    "Similarly, we'll remove `Gender`, `Married`, `Credit_History` from the data that have `NaN/null` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Gender', 'Married', 'Credit_History', 'LoanAmount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next step is creating `training` and `testing` datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_var = ['Gender','Married','Dependents','Education','Self_Employed','ApplicantIncome','CoapplicantIncome',\\\n",
    "            'LoanAmount','Loan_Amount_Term','Credit_History','Property_Area']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[pred_var], data['Loan_Status'], \\\n",
    "                                                    test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To make sure that the `pre-processing steps` are followed religiously even after we are done with experimenting and we do not miss them while predictions, we'll create a __custom pre-processing Scikit-learn `estimator`__.\n",
    "\n",
    "To follow the process on how we ended up with this `estimator`, read up on [this notebook]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom Pre-Processing estimator for our use-case\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Regular transform() that is a help for training, validation & testing datasets\n",
    "           (NOTE: The operations performed here are the ones that we did prior to this cell)\n",
    "        \"\"\"\n",
    "        \n",
    "        df = df.dropna(subset=['Gender', 'Married', 'Credit_History', 'LoanAmount']) #For Testing\n",
    "        \n",
    "        df['Dependents'] = df['Dependents'].fillna(0)\n",
    "        df['Self_Employed'] = df['Self_Employed'].fillna('No')\n",
    "        df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(self.mean_)\n",
    "        \n",
    "        gender_values = {'Female' : 0, 'Male' : 1} \n",
    "        married_values = {'No' : 0, 'Yes' : 1}\n",
    "        education_values = {'Graduate' : 0, 'Not Graduate' : 1}\n",
    "        employed_values = {'No' : 0, 'Yes' : 1}\n",
    "        property_values = {'Rural' : 0, 'Urban' : 1, 'Semiurban' : 2}\n",
    "        dependent_values = {'3+': 3, '0': 0, '2': 2, '1': 1}\n",
    "        df.replace({'Gender': gender_values, 'Married': married_values, 'Education': education_values, \\\n",
    "                    'Self_Employed': employed_values, 'Property_Area': property_values, \\\n",
    "                    'Dependents': dependent_values}, inplace=True)\n",
    "        \n",
    "        return df.as_matrix()\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        \"\"\"Fitting the Training dataset & calculating the required values from train\n",
    "           e.g: We will need the mean of X_train['Loan_Amount_Term'] that will be used in\n",
    "                transformation of X_test\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mean_ = df['Loan_Amount_Term'].mean()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert `y_train` & `y_test` to `np.array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.replace({'Y':1, 'N':0}).as_matrix()\n",
    "y_test = y_test.replace({'Y':1, 'N':0}).as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a `pipeline` to make sure that all the preprocessing steps that we do are just a single `scikit-learn estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(PreProcessing(),\n",
    "                    RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', PreProcessing()), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for the best `hyper-parameters` (`degree` for `PolynomialFeatures` & `alpha` for `Ridge`), we'll do a `Grid Search`:\n",
    "\n",
    "- Defining `param_grid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\"randomforestclassifier__n_estimators\" : [10, 20, 30],\n",
    "             \"randomforestclassifier__max_depth\" : [None, 6, 8, 10],\n",
    "             \"randomforestclassifier__max_leaf_nodes\": [None, 5, 10, 20], \n",
    "             \"randomforestclassifier__min_impurity_split\": [0.1, 0.2, 0.3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running the `Grid Search`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fitting the training data on the `pipeline estimator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessing', PreProcessing()), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impu..._jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'randomforestclassifier__n_estimators': [10, 20, 30], 'randomforestclassifier__min_impurity_split': [0.1, 0.2, 0.3], 'randomforestclassifier__max_leaf_nodes': [None, 5, 10, 20], 'randomforestclassifier__max_depth': [None, 6, 8, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see what parameter did the Grid Search select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'randomforestclassifier__n_estimators': 20, 'randomforestclassifier__min_impurity_split': 0.2, 'randomforestclassifier__max_leaf_nodes': None, 'randomforestclassifier__max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `pipeline` is looking pretty swell & fairly decent to go the most important step of the tutorial: __Serialize the Machine Learning Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the ML Model to serialize & de-serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment.\n",
    "\n",
    "In Python, `pickling` is a standard way to store objects and retrieve them as their original state. To give a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_to_pickle = [1, 'here', 123, 'walker']\n",
    "\n",
    "#Pickling the list\n",
    "import pickle\n",
    "\n",
    "list_pickle = pickle.dumps(list_to_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x03]q\\x00(K\\x01X\\x04\\x00\\x00\\x00hereq\\x01K{X\\x06\\x00\\x00\\x00walkerq\\x02e.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load the pickle back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_pickle = pickle.loads(list_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'here', 123, 'walker']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the `pickled object` to a file as well and use it. This method is similar to creating `.rda` files for folks who are familiar with `R Programming`. \n",
    "\n",
    "__NOTE:__ Some people also argue against using `pickle` for serialization[(1)](#no1). `h5py` could also be an alternative.\n",
    "\n",
    "The `grid object` would be the one that needs to be pickled in our case. Instead of `pickle` you could use `joblib` module as well to work with `pickled objects`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'model_v1.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/pratos/Side-Project/av_articles/model_v1.pk']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid, os.getcwd()+ '/' +str(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model will be saved in the location above. Now that the model `pickled`, creating a `Flask` wrapper around it would be the next step.\n",
    "\n",
    "Since, we already have the `preprocessing` steps required for the new incoming data present as a part of the `pipeline` we just have to run `predict()`. While working with `scikit-learn`, it is always easy to work with `pipelines`. If there is a section of your preprocessing or anything else in the ML Workflow that isn't already a class that's implemented, do try out [TransformerMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) to create your own estimators.\n",
    "\n",
    "`Estimators` and `pipelines` save you time and headache, even if the initial implementation seems to be ridiculous. Stich in time, saves nine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an API using Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll keep the folder structure as simple as possible:\n",
    "\n",
    "![Folder Struct](./images/flaskapp3.png)\n",
    "\n",
    "There are three important parts in constructing our wrapper function, `apicall()`:\n",
    "\n",
    "- Getting the `request` data (for which predictions are to be made)\n",
    "\n",
    "- Loading our `pickled estimator`\n",
    "\n",
    "- `jsonify` our predictions and send the response back with `status code: 200`\n",
    "\n",
    "HTTP messages are made of a header and a body. As a standard, majority of the body content sent across are in `json` format. We'll be sending (`POST url-endpoint/`) the incoming data as batch to get predictions.\n",
    "\n",
    "(__NOTE:__ You can send plain text, XML, csv or image directly but for the sake of interchangeability of the format, it is advisable to use `json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"Filename: server.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def apicall():\n",
    "    try:\n",
    "\t\ttest_json = request.get_json()\n",
    "\t\ttest = pd.read_json(test_json)\n",
    "\texcept Exception as e:\n",
    "\t\traise e\n",
    "        \n",
    "    clf = 'model_v1.pk'\n",
    "\t\n",
    "\tif test.empty:\n",
    "\t\treturn(bad_request())\n",
    "\telse:\n",
    "\t\t#Load the saved model\n",
    "\t\tloaded_model = joblib.load(os.getcwd()+'/models/'+str(clf))\n",
    "\t\tpredictions = loaded_model.predict(test)\n",
    "\t\t\n",
    "\t\tprediction_series = pd.Series(predictions)\n",
    "\t\t\n",
    "\t\t\"\"\"We can be as creative in sending the responses.\n",
    "\t\t   But we need to send the response codes as well.\n",
    "           \n",
    "           flask module provides \"jsonify\" to help create secure\n",
    "           top-level objects.\n",
    "\t\t\"\"\"\n",
    "\t\tresponses = jsonify(predictions=prediction_series.to_json())\n",
    "\t\tresponses.status_code = 200\n",
    "\n",
    "\t\treturn (responses)\n",
    "\n",
    "```\n",
    "\n",
    "Once done, run: `gunicorn --bind 0.0.0.0:8000 server:app`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some prediction data and query the API running locally at `https:0.0.0.0:8000/predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Setting the headers to send and accept json responses\n",
    "\"\"\"\n",
    "header = {'Content-Type': 'application/json', \\\n",
    "                  'Accept': 'application/json'}\n",
    "\n",
    "\"\"\"Reading test batch\n",
    "\"\"\"\n",
    "df = pd.read_csv('./flask_api/test_data/X_test.csv', header=None)\n",
    "\n",
    "\"\"\"Converting Pandas Dataframe to json\n",
    "\"\"\"\n",
    "data = df.to_json(orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"columns\":[0,1,2,3,4,5,6,7,8,9,10,11,12],\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],\"data\":[[0.11425,0.0,13.89,1.0,0.55,6.373,92.4,3.3633,5.0,276.0,16.4,393.74,10.5],[24.8017,0.0,18.1,0.0,0.693,5.349,96.0,1.7028,24.0,666.0,20.2,396.9,19.77],[0.85204,0.0,8.14,0.0,0.538,5.965,89.2,4.0123,4.0,307.0,21.0,392.53,13.83],[0.19657,22.0,5.86,0.0,0.431,6.226,79.2,8.0555,7.0,330.0,19.1,376.14,10.15],[0.10612,30.0,4.93,0.0,0.428,6.095,65.1,6.3361,6.0,300.0,16.6,394.62,12.4],[0.09178,0.0,4.05,0.0,0.51,6.416,84.1,2.6463,5.0,296.0,16.6,395.5,9.04],[4.22239,0.0,18.1,1.0,0.77,5.803,89.0,1.9047,24.0,666.0,20.2,353.04,14.64],[8.49213,0.0,18.1,0.0,0.584,6.348,86.1,2.0527,24.0,666.0,20.2,83.45,17.64],[0.17899,0.0,9.69,0.0,0.585,5.67,28.8,2.7986,6.0,391.0,19.2,393.29,17.6],[0.537,0.0,6.2,0.0,0.504,5.981,68.1,3.6715,8.0,307.0,17.4,378.35,11.65],[0.09378,12.5,7.87,0.0,0.524,5.889,39.0,5.4509,5.0,311.0,15.2,390.5,15.71],[0.06162,0.0,4.39,0.0,0.442,5.898,52.3,8.0136,3.0,352.0,18.8,364.61,12.67],[14.4383,0.0,18.1,0.0,0.597,6.852,100.0,1.4655,24.0,666.0,20.2,179.36,19.78],[0.09512,0.0,12.83,0.0,0.437,6.286,45.0,4.5026,5.0,398.0,18.7,383.23,8.94],[5.70818,0.0,18.1,0.0,0.532,6.75,74.9,3.3317,24.0,666.0,20.2,393.07,7.74],[0.33147,0.0,6.2,0.0,0.507,8.247,70.4,3.6519,8.0,307.0,17.4,378.95,3.95],[0.31533,0.0,6.2,0.0,0.504,8.266,78.3,2.8944,8.0,307.0,17.4,385.05,4.14],[0.30347,0.0,7.38,0.0,0.493,6.312,28.9,5.4159,5.0,287.0,19.6,396.9,6.15],[1.42502,0.0,19.58,0.0,0.871,6.51,100.0,1.7659,5.0,403.0,14.7,364.31,7.39],[0.0136,75.0,4.0,0.0,0.41,5.888,47.6,7.3197,3.0,469.0,21.1,396.9,14.8],[11.1604,0.0,18.1,0.0,0.74,6.629,94.6,2.1247,24.0,666.0,20.2,109.85,23.27],[0.04819,80.0,3.64,0.0,0.392,6.108,32.0,9.2203,1.0,315.0,16.4,392.89,6.57],[0.04417,70.0,2.24,0.0,0.4,6.871,47.4,7.8278,5.0,358.0,14.8,390.86,6.07],[0.04741,0.0,11.93,0.0,0.573,6.03,80.8,2.505,1.0,273.0,21.0,396.9,7.88]]}'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"POST <url>/predict\n",
    "\"\"\"\n",
    "resp = requests.post(\"http://0.0.0.0:8000/predict\", \\\n",
    "                    data = json.dumps(data),\\\n",
    "                    headers= header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': '{\"0\":24.8240379255,\"1\":11.8654385585,\"2\":17.1561912854,\"3\":20.0458076619,\"4\":19.2367642081,\"5\":25.8492084053,\"6\":22.9006678292,\"7\":15.9280178003,\"8\":21.4350486365,\"9\":22.023735047,\"10\":21.2012363118,\"11\":18.5667463494,\"12\":15.8056937572,\"13\":21.9883614457,\"14\":28.400726322,\"15\":42.9079207048,\"16\":43.1399770618,\"17\":24.3422675202,\"18\":26.679499558,\"19\":19.185684845,\"20\":10.3816598089,\"21\":19.9347015253,\"22\":27.2866216946,\"23\":18.8497692623}'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The final response we get is as follows:\n",
    "\"\"\"\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have half the battle won here, with a working API that serves predictions in a way where we take one step towards integrating our ML solutions right into our products. This is a very basic API that will help with proto-typing a data product, to make it as fully functional, production ready API a few more additions are required that aren't in the scope of Machine Learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sources & Links:__\n",
    "\n",
    "[1]. <a id='no1' href=\"http://www.benfrederickson.com/dont-pickle-your-data/\">Don't Pickle your data.</a>\n",
    "\n",
    "[2]. <a id='no2' href=\"http://www.dreisbach.us/articles/building-scikit-learn-compatible-transformers/\">Building Scikit Learn compatible transformers.</a>\n",
    "\n",
    "[3]. <a id='no2' href=\"http://flask.pocoo.org/docs/0.10/security/#json-security\">Using jsonify in Flask.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
